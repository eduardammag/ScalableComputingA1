\documentclass[a4paper,12pt]{article}

% Pacotes úteis
\usepackage[utf8]{inputenc}       % Codificação UTF-8
\usepackage[T1]{fontenc}          % Suporte para acentuação
\usepackage{lmodern}              % Fonte moderna
\usepackage{graphicx}             % Para inclusão de imagens
\usepackage{amsmath}              % Pacote de matemática
\usepackage{amssymb}              % Símbolos matemáticos
\usepackage{geometry}             % Controle das margens
\usepackage{setspace}             % Controle de espaçamento
\usepackage{indentfirst}          % Indentar primeiro parágrafo de cada seção
\usepackage{hyperref}             % Links clicáveis
\usepackage{float}                % Controle de posição de elementos flutuantes (figuras, tabelas)
\usepackage{titlesec}             % Personalização de títulos
         % Para imagens (caso use)
\usepackage{xcolor}           % Para cores nos textos (opcional)
\usepackage{listings}         % Para código C++ (se for incluir trechos)



\documentclass{article}
\usepackage{minted}

% Configurações opcionais para personalizar
\setminted{
    breaklines=true,      % Quebra automaticamente linhas longas
    breakanywhere=true,   % Permite quebrar palavras (opcional)
    frame=lines,          % Adiciona linhas ao redor do código
    fontsize=\small,      % Tamanho da fonte
    breaklines=true,
    breaksymbol=,         % Adiciona um símbolo (opcional)
    breakindent=10pt       % Indenta as linhas quebradas
}



% Configurações de página
% \geometry{left=3cm, right=2cm, top=2cm, bottom=2cm}
% \onehalfspacing                      % Espaçamento de 1,5 entre linhas

% % Cabeçalho com título e autores
% \title{Avaliação de LLM's na extração de dados médicos de notas clínicas}
% \author{
%     Ana Júlia Amaro Pereira Rocha \\ 
%     Maria Eduarda Mesquita Magalhães\\ 
%     Mariana Fernandes Rocha \\ 
%     Paula Eduarda de Lima \\
%     \\ Orientador: Walter Sande
% }
% \date{\today}                         % Data do relatório

% \begin{document}

% \documentclass[12pt,a4paper]{report}
\usepackage{graphicx}
\usepackage{titling}

\documentclass[a4paper,12pt]{article}

\usepackage[portuguese]{babel} % Ativa o português para hifenização
\usepackage[utf8]{inputenc}    % Suporte a caracteres acentuados
\usepackage[T1]{fontenc}       % Fonte com suporte a caracteres especiais

\begin{document}

\begin{titlepage}
    \begin{center}

        \vspace{1cm}
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=1.2\textwidth]{logo_fgv.png}    
        \end{minipage}
        \vspace{2cm}

        \rule{1\textwidth}{0.4pt} \\ % Linha horizontal personalizada
        \vspace{0.3cm}
        {\Huge \textbf{Projeto de Micro-Framework}} \\
        \vspace{0.2cm}
        \vspace{0.5cm}\\
        {\Large \textbf{A1 Computação Escalável}}\\
        \rule{1\textwidth}{0.4pt} % Linha horizontal personalizada


        \vspace{0.5cm}
        {\Large \textbf{FGV EMAp}} \\
        \vspace{2cm}
        
        

        
        
        % % Unidade e curso
        % {\Large \textbf{FGV EMAp}}\\[2cm]
        
        % Autores
        {\large 
            \textbf{Ana Júlia Amaro Pereira Rocha} \\ 
            \textbf{Maria Eduarda Mesquita Magalhães}\\
            \textbf{Mariana Fernandes Rocha} \\
            \textbf{Paula Eduarda de Lima}}\\[1.5cm]
        
        % Informações adicionais
        {\large 
            Ciência de Dados e Inteligência Artificial \\ 
            5º Período}\\[2cm]
        
         % Data
        \vfill
        {\large Rio de Janeiro, 2025}

        
    \end{center}
\end{titlepage}

\tableofcontents
\newpage


\section{Introdução}
Este relatório apresenta o desenvolvimento de um Micro-Framework voltado à construção de pipelines para processamento de dados. O principal objetivo é fornecer uma solução que promova maior eficiência e balanceamento de carga, por meio da execução concorrente e paralela das etapas de processamento. Embora a aplicação tenha sido especialmente construida para o monitoramento de uma doença infecciosa, sua estrutura foi projetada de forma genérica, com pontos de extensão (hot-spots) que permitem a personalização conforme as necessidades específicas de diferentes cenários.

\section{Modelagem}

\begin{figure}[H]
    \hspace{-1cm}
    \includegraphics[width=1.15\linewidth]{_Fluxograma.jpeg}
    \caption{Fluxograma do framework}
    \label{fig:minha_imagem}
\end{figure}

A modelagem, representada no fluxograma, é feita de um pipeline de processamento de dados baseado em um fluxo ETL (Extract, Transform, Load), acionado por diferentes triggers, organizado em etapas independentes, que se comunicam por meio de filas. O processo se inicia por um módulo mock que simula a chegada de dados reais de fontes distintas. Esses dados são armazenados em arquivos e enviados para a primeira etapa do pipeline.

Quando o pipeline é ativado por um dos triggers, os arquivos são encaminhados para o extrator, que é responsável por identificar o tipo de arquivo e padronizar os dados extraídos em uma estrutura de dataframe. Esse processo garante que, independentemente do formato de origem, todos os dados estejam organizados de maneira uniforme para as próximas etapas do pipeline.

Após a extração, os dados passam por uma cadeia de tratadores gerais, que realizam limpezas e filtragens padronizadas, T1, T2 - geral.

Na sequência, os dados seguem para tratadores específicos, que aplicam transformações mais contextualizadas. O T3 - específico concatena diferentes DataFrames, unificando tabelas com a mesma estrutura. O T4 - específico realiza agrupamentos por colunas de interesse (como CEP ou hospital), preparando os dados para análises agregadas. Por fim, o T5 - específico calcula médias e emite um alerta, oferecendo uma visão resumida dos dados processados.

Após o tratamento, os dados são encaminhados para o Loader, responsável por carregá-los em arquivos csv's. Isso garante que os dados tratados fiquem disponíveis para consultas futuras e possam ser utilizados por módulos de visualização. Por fim, o componente Display consome essas informações armazenadas e as exibe relatórios semanais, fechando o ciclo do pipeline. 

\section{Mock}

As fontes de dados serão simulações de hospitais,  Secretaria de Saúde (SS) e Organização Mundial da Saúde (OMS). Receberemos conjuntos de dados advindos dessas fontes semanalmente. Além disso, temos dois tipos de segmentação regional: Ilhas, mais geral, e regiões, que são segmentações das ilhas, mais especifíco. A simulação apresenta 20 ilhas, cada uma identificada por um CEP de 2 dígitos, cada ilha tem 5 regiões, com CEPs de 5 dígitos, sendo os dois primeiros dígitos o CEP da ilha (ex: ilha 12 $\rightarrow$ regiões 12001 a 12005).


O objeto Mock é feito em python e implementa a criação de três tipos de tabelas diferes: SQLite, csv e txt.
\vspace{0.5cm}

\textbf{Fontes e dados gerados}

\begin{itemize}
    \item \textcolor{purple}{\textbf{OMS (\texttt{oms\_mock.txt})}}\\
    Dados agregados por ilha (CEP de 2 dígitos).\\
    Inclui número de óbitos, população, recuperados, vacinados e data.\\
    Gerado em formato \texttt{.txt} com tabulações.
    
    \vspace{0.5em}
    
    \item \textcolor{red}{\textbf{Hospitais (\texttt{hospital\_mock\_*.csv})}}\\
    Dados individuais por paciente, com CEPs de 5 dígitos (regiões).\\
    Contém informações como internação, idade, sexo, sintomas e data.\\
    Gera múltiplos arquivos \texttt{.csv}, simulando diferentes hospitais.
    
    \vspace{0.5em}
    
    \item \textcolor{orange}{\textbf{Secretaria de Saúde (\texttt{secretary\_data.db})}}\\
    Banco de dados SQLite com tabela \texttt{pacientes}.\\
    Dados por região (CEP de 5 dígitos): diagnóstico, vacinação, escolaridade, população e data.\\
    Registros inseridos diretamente em um banco relacional.
\end{itemize}

\section{Triggers}
O framework apresenta dois tipos de Triggers responsáveis por iniciar a execução de um pipeline:

\begin{itemize}
    \item \textbf{TimerTrigger}: A cada X segundos dispara uma nova execução.
    \item \textbf{RequestTrigger}: A cada chamada de função (simulando uma requisição de rede) dispara uma nova
execução.
\end{itemize}

\section{Extrator}

O  extrator, implementado em C++, usa a classe \texttt{DataFrame} que organiza os dados gerados pelo mock em uma estrutura tabular com colunas bem definidas e suporta os diferentes tipos de dados (inteiros, reais e strings). Ele permite:

\begin{itemize}
    \item \textbf{Validar os dados inseridos:} garante que cada valor esteja no formato correto (ex: inteiros em colunas inteiras).
    
    \item \textbf{Adicionar ou remover linhas e colunas:} facilita manipulações e filtragens nos dados carregados.
    
    \item \textbf{Visualizar os dados em forma de tabela:} com o método \texttt{display}, os dados são impressos com cabeçalho e valores formatados.
    
    \item \textbf{Acessar partes específicas:} como uma linha (\texttt{getRow}) ou coluna (\texttt{colIdx} e \texttt{typeCol}).
    
    \item \textbf{Verificar informações básicas:} número de colunas (\texttt{numCols}), número de linhas (\texttt{size}), se está vazio (\texttt{empty}), etc.
\end{itemize}

\vspace{1em}

\textbf{Aplicação com os dados gerados}
\\
\\
O extrator identifica o tipo de arquivo da tabela em questão por meio da extensão pós ponto (.csv,.txt ou .db) e, com isso, chama a respectiva função de extração para esse tipo de dado, tornando os diferentes tipos de tabelas padronizadas para o uso dos tratadores.\\
\\
Quando os dados da OMS (arquivo \texttt{.txt}) ou dos hospitais (arquivos \texttt{.csv}) são carregados, o \texttt{DataFrame} pode ser inicializado com os nomes das colunas e seus tipos, e cada linha é validada antes de ser inserida.
\\
\\
O extrator também pode ser usado para representar tabelas extraídas do banco de dados SQLite da Secretaria de Saúde (por exemplo, a tabela \texttt{pacientes}).
\\
\\
Ele garante integridade dos dados e fornece uma base comum para as análises posteriores.


\section{Tratadores}

Os tratadores implementados em C++ nesse projeto são responsáveis por preparar os dados para análises. Foram implementados 5 tratadores, sendo 2 gerais (todos os arquivos passam por eles) e 3 específicos:

\begin{itemize}
    \item \textbf{T1 - geral:} O tratador 1 é responsável por eliminar duplicatas, além de colunas e linhas majoritariamente nulas. Ele recebe um dataframe vindo do extrator e retorna um dataframe para o tratador 2.

    \item \textbf{T2 - geral:} O tratador 2 é responsável por validar os dados do dataframe, isto é, eliminar linhas com idade negativa, números de CEP com menos ou mais dígitos do que deveria, número total de óbitos negativo, entre outros. Este tratador também tem grau de entrada e saída 1.
    
    \item \textbf{T3 - específico:} O tratador 3 tem grau de entrada 3 e grau de saída 4. Ele recebe 3 dataframes com duas colunas (vindos do T4 que é detalhado a seguir) e faz o merge dos três dataframes e de suas permutações também, isto é, considerando A, B e C os DFs que entram, temos que serão retornados os DFs AB, AC, BC e ABC. Tudo isso acontece por meio de uma coluna em comum a qual no escopo deste projeto é aquela referente ao CEP (única coluna que se repete em todos os DFs, ainda que em alguns seja CEP da ilha e em outros o CEP de uma região de uma ilha, mas essas diferenças foram tratadas observando apenas os 2 primeiros dígitos os quais se referem à ilha).
    
    \item \textbf{T4 - específico:} O tratador 4 recebe um Dataframe limpo do T2, uma coluna de agregação e uma coluna de agrupamento. Basicamente, é retornado um Dataframe com as duas colunas especificadas, sendo que a coluna de agregação terá seus dados agrupados de acordo com a outra coluna mencionada. Por exemplo, se a ideia é descobrir o número de casos por região, o T4 retorna um DF com a coluna referente ao número de casos e a coluna referente ao CEP da região que possuirá apenas valores únicos com o agrupamento. O DF de saída pode ir para o T3 ou direto para o Loader.
    
    \item \textbf{T5 - específico:} O tratador 5 recebe um DF limpo do T2 e é responsável por calcular a média dos dados em uma determinada coluna, criando uma coluna de alerta para indicar quais linhas estão acima da média. Por exemplo, se a ideia é identificar regiões com surtos de epidemia pode-se calcular a média do número de internados e gerar um alerta vermelho para os hospitais e/ou regiões que estejam com mais internações que o "normal" pela doença infecciosa a ser monitorada.
    
\end{itemize}

\section{Loader}

O loader tem uma função bastante simples que é enviar os dataframes tratados para o banco de dados fazendo com que os dados estejam disponíveis de forma mais confiável do que quando chegaram ao pipeline para futuros acessos e análises. Portanto, neste projeto, o loader recebe os dataframes tratados, transforma-os em CSV's e salva no banco de dados que no caso é apenas uma pasta chamada de database loader. A partir disso, pode-se fazer as análises a serem exibidas no dashboard para monitoramento da doença infecciosa em questão no projeto.


\section{Dashboard}

O dashboard é composto por cinco análises principais, cada uma com foco em aspectos distintos da saúde pública nas ilhas monitoradas. Temos 5 análises:

\begin{itemize}
    \item \textbf{Análise 1: Alerta semanal por CEP} \\
    Esta análise processa os arquivos já tratados da OMS contendo alertas por CEP. Um alerta vermelho é emitido para os CEPs que apresentaram cinco ou mais ocorrências de alerta \texttt{True} ao longo da semana, enquanto os demais são classificados com alerta verde. O objetivo é sinalizar áreas críticas de forma direta e sumarizada.

    \item \textbf{Análise 2: Estatísticas globais de internações} \\
    Aqui, agregamos os dados de internação de todos os hospitais e calculamos estatísticas gerais como média e desvio padrão do número total de internados. O processamento é realizado de forma paralela utilizando múltiplas threads para ganho de desempenho.

    \item \textbf{Análise 3: Estatísticas por hospital} \\
    Similar à Análise 2, mas os cálculos são feitos individualmente por hospital. Para cada hospital, são reportadas a média e o desvio padrão dos internados. Essa análise permite identificar hospitais com padrões anormais em relação ao restante.

    \item \textbf{Análise 4: Correlação entre vacinação e óbitos} \\
    Aqui, calculamos a correlação de Pearson entre o total de vacinados e o número de óbitos registrados, utilizando dados de um arquivo resultando do merge. O objetivo é avaliar se há relação linear entre o avanço da vacinação e a mortalidade.
    
    \item \textbf{Análise 5: Regressão linear entre vacinação e internações} \\
    Nesta análise fizemos uma regressão linear simples para estimar o número de internados com base na quantidade de vacinados. A intenção é investigar se o aumento na vacinação influencia a redução ou aumento nas internações.
\end{itemize}

\section{Pipeline}
O Pipeline é feito de modo a organizar o fluxo de trabalho de maneira eficiente e paralela. O objetivo principal é o passo a passo dejado do ETL e a escolha da ordem e dos diferentes tratadores desejados. Além disso,essa arquitetura pode ser adaptada para qualquer outro contexto de processamento de dados.

\subsection*{Lógica Produtor-Consumidor}
De maneira geral, o paralelismo no pipeline se dá pela lógica de Produtor-Consumidor, da seguinte forma:
\\
\\
\textbf{Produtor}: gera dados e os insere em uma fila compartilhada.
\\
\\
\textbf{Consumidor}: retira dados dessa fila para processá-los.
\\
\\
\textbf{Fila compartilhada}: estrutura de dados para comunicação entre threads.
\\
\\
\textbf{Mutex (trava)}: garante que só uma thread acesse a fila por vez.
\\
\\
\textbf{Condition Variable}: notifica consumidores quando há dados disponíveis (ou produtores quando há espaço, se necessário).



\subsection*{Extração}
O paralelismo na extração dos arquivos gerados pelo mock no pipeline é implementado com base no modelo produtor-consumidor utilizando threads, filas protegidas por mutex, e variáveis de condição para sincronização.

\begin{itemize}
    \item Produtor coloca arquivos na fila (filaArquivos).

    \item  Múltiplos consumidores-extratores (threads) consomem essa fila paralelamente (o número de consumidores é um argumento da função pipeline, definido como preferível).

    \item  Cada consumidor-extrator:
    \begin{itemize}
        \item Lê o arquivo (pode ser .csv, .txt, ou .db).
        \item Constrói um DataFrame.
        \item Envia esse DataFrame para a próxima etapa (extratorTratadorFila), que será tratada por outra thread.
    \end{itemize}

\end{itemize}

Múltiplas threads \texttt{consumidorExtrator} são criadas (na função \texttt{executarPipeline}).

Cada thread opera de forma independente, competindo por arquivos na fila \texttt{filaArquivos}, gerando uma condição de corrida, corrigida por uma proteção da fila com mutex.
 
Isso permite que vários arquivos sejam processados \textbf{simultaneamente} por diferentes núcleos da CPU e haja um balanceamento de carga.
\\
\subsubsection*{Pontos fortes da abordagem}
\\
\begin{itemize}
    \item \textbf{Escalabilidade com threads:} permite processar múltiplos arquivos simultaneamente, aproveitando melhor os múltiplos núcleos da CPU, o que acelera a etapa de extração.

    \item \textbf{Balanceamento automático de carga:} como os consumidores competem por arquivos na fila, a distribuição é feita de forma natural e eficiente, sem divisão manual de tarefas.

    \item \textbf{Desacoplamento entre etapas:} o uso de filas entre as fases (extração, tratamento e carregamento) permite que cada parte do pipeline opere de forma independente, evitando gargalos.

    \item \textbf{Modelo confiável:} o padrão produtor-consumidor com mutex e variáveis de condição é amplamente utilizado e considerado seguro para aplicações concorrentes.

    \item \textbf{Paralelismo configurável:} o número de consumidores pode ser definido por parâmetro na execução, facilitando testes e adaptações ao ambiente de hardware.

    \item \textbf{Sincronização segura:} uso correto de \texttt{mutex} e \texttt{condition\_variable} impede condições de corrida no acesso às filas compartilhadas.
\end{itemize}

\vspace{1em}

\\
\subsubsection*{Pontos fracos da abordagem}

\begin{itemize}
    \item \textbf{Overhead de gerenciamento de threads:} quando há muitos arquivos pequenos, o custo de criação e sincronização de threads pode superar os ganhos do paralelismo.

    \item \textbf{Ausência de priorização de arquivos:} a fila segue uma lógica FIFO simples, não considerando arquivos urgentes ou mais pesados que poderiam ser tratados primeiro.

    \item \textbf{Fila sem limite de tamanho:} caso a leitura seja muito mais rápida que o tratamento, a fila intermediária pode crescer indefinidamente e consumir muita memória.

    \item \textbf{Threads ociosas em cenários com poucos arquivos:} se a carga for baixa, as threads de extração encerram rapidamente e não são reaproveitadas.

    \item \textbf{Bloqueio com variáveis de condição:} consumidores ficam bloqueados esperando arquivos, o que pode causar ineficiência em cenários com picos esparsos de chegada de dados.

\end{itemize}

\subsection*{Tratamento}
Na etapa de tratamento, a lógica produtor-consumidor se organiza da seguinte forma:

\begin{itemize}
    \item \textbf{Produtor:} as threads consumidoras da etapa de extração enviam os DataFrames processados para a fila extratorTratadorFila, que serve como ponte para o tratamento.

    \item \textbf{Consumidores:} múltiplas threads tratadoras (criadas na função executarPipeline) consomem os DataFrames dessa fila.

    \item Cada consumidor-tratador:
    \begin{itemize}
        \item Executa o tratamento dos dados conforme a lógica definida (e.g., limpeza, transformação).

        \item Insere os dados tratados na fila tratadorLoaderFila, que será consumida pela próxima etapa.
        
    \end{itemize}

\end{itemize}

\subsubsection*{Pontos fortes específicos da abordagem}
\\

\begin{itemize}
    \item \textbf{} 
    
\end{itemize}
Além da escalabilidade, balanceamento e desacoplamento detalhados entre os pontos fortes do paralelismo na extração.

\vspace{1em}

\\
\subsubsection*{Pontos fracos específicos da abordagem}

\begin{itemize}
    \item \textbf{} 

\end{itemize}


\subsection*{Loader}
\\
Na etapa de carregamento (loader), o paralelismo também segue a lógica produtor-consumidor:

\begin{itemize}
    \item \textbf{Produtor:} as threads de tratamento inserem os dados tratados na fila tratadorLoaderFila.

    \item \textbf{Consumidores:} múltiplas threads loader consomem essa fila para enviar os dados para o destino final (e.g., banco de dados).

    \item Cada thread loader:
    \begin{itemize}
        \item Lê o item da fila.
        \item Realiza a operação de carga dos dados.
    \end{itemize}
\end{itemize}


\subsubsection*{Pontos fortes específicos da abordagem}
\\
\begin{itemize}
    \item \textbf{Aproveitamento de I/O paralelamente:} carregar dados em paralelo pode reduzir o tempo total quando o destino (como um banco) suporta múltiplas conexões simultâneas.
    
\end{itemize}
Além da escalabilidade, balanceamento e desacoplamento detalhados entre os pontos fortes do paralelismo na extração.

\vspace{1em}

\\
\subsubsection*{Pontos fracos específicos da abordagem}

\begin{itemize}
    \item \textbf{Concorrência com I/O externo:} se o destino de carga for um recurso compartilhado (como um banco com limite de conexões), muitas threads podem gerar contenção.

    \item \textbf{Dificuldade de controle de erro e rollback:} paralelismo em carregamento pode dificultar o rastreamento e recuperação de falhas, especialmente se os dados forem carregados em lotes ou transações distintas.


\end{itemize}

\section{Análise de tempo de execução}
\end{document}
